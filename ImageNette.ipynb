{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9a9m5NyL-yc"
   },
   "source": [
    "<h1>DISCERN</h1>\n",
    "<h2>Diversity-based Selection of Centroids and k-Estimation for Rapid Non-stochastic clustering</h2>\n",
    "<h3>Applying DISCERN to an image dataset using ConvNet embeddings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running this notebook requires PyTorch (torchvision + CUDA). Please note that you may need to download the pretrained model, and the dataset beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDZeFhGPMAOu"
   },
   "source": [
    "<h3>1. Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKQrIkO1MQoQ"
   },
   "source": [
    "We're going to do something a bit different from the paper, which is to use a pretrained MoCo which is trained in unlabled data using contrastive loss. We highly recommend you check out <a href=\"https://arxiv.org/abs/1911.05722\">the paper</a> as well as the <a href=\"https://github.com/facebookresearch/moco/\">GitHub repo</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DB2SL-LHNKnH"
   },
   "outputs": [],
   "source": [
    "pretrained_path = \"moco_v2_800ep_pretrain.pth.tar\" ####Link available in MoCo's GitHub Repo####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xXCTBlDEL8eK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from DISCERN import DISCERN\n",
    "from utils import purity_score as purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the weights of the MoCo query encoder from the checkpoint we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PK4S9SSKM4uE"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50()\n",
    "checkpoint = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "\n",
    "state_dict = checkpoint['state_dict']\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
    "        state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
    "    del state_dict[k]\n",
    "\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "model.fc = nn.Identity()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTLtfaOuRXFA"
   },
   "source": [
    "<h3>2. Import data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try <a href=\"https://github.com/fastai/imagenette\">ImageNette</a>, which was mentioned in the paper. The only difference is that we used an labeled ImageNet-pretrained ResNet101 then and here we're using a ResNet50 which was trained in a completely unsupervised setting (MoCo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following was done on a single Cloud GPU (Tesla T4), so you may need to adjust the batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hXc4Gk74NwDE"
   },
   "outputs": [],
   "source": [
    "tr = transforms.Compose([\n",
    "                         transforms.Resize((224, 224)),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\"imagenette2-320/train\", tr)\n",
    "valset = torchvision.datasets.ImageFolder(\"imagenette2-320/val\", tr)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1024, shuffle=False, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=1024, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use the MoCo-trained ResNet50 model to get the latent embeddings of the training and validation sets into numpy ndarrays so that DISCERN can process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JNx4A7C2PP-C"
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(trainset), 2048))\n",
    "X_val = np.zeros((len(valset), 2048))\n",
    "\n",
    "y_train = np.zeros((len(trainset)), dtype=int)\n",
    "y_val = np.zeros((len(valset)), dtype=int)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ctr = 0\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        output = model(images)\n",
    "        ctr_new = ctr + images.shape[0]\n",
    "        X_train[ctr:ctr_new, :] = output.cpu().numpy()\n",
    "        y_train[ctr:ctr_new] = target.numpy()\n",
    "        ctr = ctr_new\n",
    "    ctr = 0\n",
    "    for i, (images, target) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "        output = model(images)\n",
    "        ctr_new = ctr + images.shape[0]\n",
    "        X_val[ctr:ctr_new, :] = output.cpu().numpy()\n",
    "        y_val[ctr:ctr_new] = target.numpy()\n",
    "        ctr = ctr_new\n",
    "\n",
    "\n",
    "num_class = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kUZEnH1gTtfm",
    "outputId": "c889cbf3-073e-4f70-9383-e0183661f492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9469 training samples, 3925 validation samples\n"
     ]
    }
   ],
   "source": [
    "print(\"{} training samples, {} validation samples\".format(X_train.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq8SS7sXQ7CT"
   },
   "source": [
    "<h3>3. Running DISCERN</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running DISCERN with a limit on the number of clusters it can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2sWFKxJtQ2hw"
   },
   "outputs": [],
   "source": [
    "d = DISCERN(max_n_clusters=100)\n",
    "d.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n_-z2wzBRIg7"
   },
   "outputs": [],
   "source": [
    "c_val = d.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T_tcnNs4VlN0"
   },
   "outputs": [],
   "source": [
    "val_accuracy = purity(y_val, c_val)*100\n",
    "num_clusters = len(np.unique(d.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cE9ROwvxV7zL",
    "outputId": "2227a32d-fd30-4d16-ad87-56fb1fff1f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Supervised   Performance] Accuracy: 50.318471337579616 %\n",
      "Predicted number of clusters: 7\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"[Supervised   Performance] Accuracy: {} %\".format(val_accuracy))\n",
    "print(\"Predicted number of clusters: {}\".format(num_clusters))\n",
    "print(\"Number of classes: {}\".format(num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVUB7_i5cpIO"
   },
   "source": [
    "As it can be seen, it is not perfect. When you apply PCA, you can further improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "STefdILkb3IH"
   },
   "outputs": [],
   "source": [
    "X_train_ds = PCA(1024, tol=1e-10, random_state=0).fit_transform(X_train)\n",
    "X_val_ds = PCA(1024, tol=1e-10, random_state=0).fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xVJag8vUc2OU"
   },
   "outputs": [],
   "source": [
    "d2 = DISCERN(max_n_clusters=100)\n",
    "d2.fit(X_train_ds)\n",
    "c_val_ds = d2.predict(X_val_ds)\n",
    "val_accuracy_ds = purity(y_val, c_val_ds)*100\n",
    "num_clusters_ds = len(np.unique(d2.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6vT4iY4VdA0g",
    "outputId": "99fe6b69-5d9a-452f-dd0e-ebbb7eeb5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Supervised   Performance] Accuracy: 67.87261146496816 %\n",
      "Predicted number of clusters: 13\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"[Supervised   Performance] Accuracy: {} %\".format(val_accuracy_ds))\n",
    "print(\"Predicted number of clusters: {}\".format(num_clusters_ds))\n",
    "print(\"Number of classes: {}\".format(num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f54ozfqdDcS"
   },
   "source": [
    "<b>Note that NO LABELS were used to achieve these results, as MoCo was trained in a completely unsupervised setting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYgqpMmbdbQT"
   },
   "source": [
    "DISCERN has so far performed best on semi-supervised-trained networks such as FaceNet for object re-identification. However, its complexity remains an issue which we're looking forward to tackle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}